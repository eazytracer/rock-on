---
title: Comprehensive Test Strategy - MVP Pre-Launch
created: 2025-11-03T15:20
context: Post-audit migration, pre-MVP launch testing focus
prompt: Review test coverage, identify edge cases, design tests for user journeys and broken behavior patterns (session timeouts, etc.)
status: Strategic Plan
---

# ğŸ¯ Comprehensive Test Strategy - MVP Pre-Launch

**Created:** 2025-11-03T15:20
**Context:** User discovered edge cases (session timeouts causing broken behavior) and wants comprehensive test coverage before MVP launch
**Goal:** 100% passing tests with real edge case coverage, not just happy path testing

---

## ğŸ“Š Current Reality Check

### Actual Test Status (as of 2025-11-03)
```
Test Files:  17 failed | 27 passed (44 total)
Tests:       69 failed | 555 passed (633 total)
Pass Rate:   87.7%
```

**NOT** the 98.2% with 8 failures from the old plan! The plan is outdated.

### What's Actually Passing âœ… (555 tests)
- âœ… SyncEngine core logic (21/21)
- âœ… RemoteRepository (13/13)
- âœ… LocalRepository
- âœ… Database utilities
- âœ… Most sync infrastructure tests
- âœ… Core CRUD operations
- âœ… Version tracking
- âœ… Audit system basics

### What's Actually Failing âŒ (69 tests across 17 files)

**Category 1: Mock Setup Issues (NOT code bugs)**
- RealtimeManager.test.ts: 8 failures
  - Issue: Mock doesn't create unique channel instances
  - Impact: Test infrastructure problem, not production code

**Category 2: Database Cleanup Issues**
- Integration tests (optimistic-updates.test.ts): 6 failures
  - Issue: "DatabaseClosedError" - cleanup timing problems
  - Impact: Test infrastructure, not production code

**Category 3: React Testing Setup**
- PracticesPage.test.tsx: Multiple failures
- Hook tests: Provider/context setup issues
- Impact: Test infrastructure, not production code

**Category 4: Unknown** (51 other failures)
- Need detailed analysis to categorize

---

## ğŸš¨ The Real Problem: Testing the Wrong Things

### What Current Tests Do
âŒ Test implementation details (mock calls, internal state)
âŒ Test architecture (table-based vs audit-first)
âŒ Test happy paths only
âŒ Written to pass, not to catch bugs

### What Current Tests DON'T Do
âœ… Test real user journeys
âœ… Test edge cases (session timeouts, network failures)
âœ… Test race conditions
âœ… Test error recovery
âœ… Test data consistency across failures
âœ… Test concurrent operations

### User-Reported Edge Cases (The Real Issues!)
1. **Session timeout** â†’ Broken app behavior
2. **Network failure** during sync â†’ Data loss?
3. **Multiple tabs** open â†’ Conflicting state?
4. **Long-running session** â†’ Memory leaks?
5. **Rapid successive changes** â†’ Queue overflow?
6. **Large datasets** â†’ Performance degradation?

**These are NOT tested by current suite!**

---

## ğŸ¯ New Testing Strategy: Journey-First, Edge-Case-Focused

### Principle 1: Test Behavior, Not Implementation
```typescript
// âŒ BAD: Tests implementation
it('should call mockSupabase.channel with correct params', () => {
  expect(mockSupabase.channel).toHaveBeenCalledWith('audit-band-1')
})

// âœ… GOOD: Tests behavior
it('should sync new song to all connected devices within 1 second', async () => {
  // Create song in device 1
  await device1.createSong({ title: 'New Song' })

  // Verify appears in device 2
  await waitFor(() => {
    const songs = device2.getSongs()
    expect(songs).toContainEqual(expect.objectContaining({ title: 'New Song' }))
  }, { timeout: 1000 })
})
```

### Principle 2: Test User Journeys, Not Isolated Functions
```typescript
// âŒ BAD: Tests isolated function
it('should convert song to Supabase format', () => {
  const result = toSupabaseFormat(song)
  expect(result.tempo).toBe(120)
})

// âœ… GOOD: Tests complete journey
it('JOURNEY: Musician creates song offline, goes online, sees it sync', async () => {
  // 1. Start offline
  await goOffline()

  // 2. Create song
  const song = await createSong({ title: 'Offline Song', bpm: 120 })
  expect(song).toBeDefined()

  // 3. Verify in local DB only
  expect(await localDB.getSong(song.id)).toBeDefined()
  expect(await supabase.getSong(song.id)).toBeNull()

  // 4. Go online
  await goOnline()

  // 5. Wait for sync
  await waitForSync()

  // 6. Verify in both DBs with correct format
  const local = await localDB.getSong(song.id)
  const remote = await supabase.getSong(song.id)

  expect(local.bpm).toBe(120) // camelCase
  expect(remote.tempo).toBe(120) // snake_case
  expect(local.title).toBe(remote.title)
})
```

### Principle 3: Test Edge Cases First
```typescript
// âœ… Test the edge cases that break production
describe('Edge Cases - Session Timeout', () => {
  it('should gracefully handle session timeout during sync', async () => {
    // Create song
    const song = await createSong({ title: 'Test' })

    // Simulate session timeout
    await simulateSessionTimeout()

    // Try to sync
    await triggerSync()

    // Should show auth error, not crash
    expect(screen.getByText(/session expired/i)).toBeInTheDocument()

    // Should preserve local data
    expect(await localDB.getSong(song.id)).toBeDefined()

    // Should retry after re-auth
    await reAuthenticate()
    await waitForSync()
    expect(await supabase.getSong(song.id)).toBeDefined()
  })
})
```

---

## ğŸ“‹ Recommended Test Categories (Priority Order)

### P0: Critical User Journeys (Must Have Before MVP)
**Time: 8-10 hours**

1. **Authentication Journeys** (2 hours)
   - âœ… Login â†’ Use app â†’ Session timeout â†’ Re-auth â†’ Continue
   - âœ… Login â†’ Multiple tabs â†’ One logs out â†’ Others handle it
   - âœ… Login â†’ Close tab â†’ Reopen â†’ Session persists
   - âœ… Quick login (dev mode) works

2. **Offline/Online Sync Journeys** (3 hours)
   - âœ… Online â†’ Create data â†’ Offline â†’ Data still accessible
   - âœ… Offline â†’ Create data â†’ Online â†’ Data syncs to cloud
   - âœ… Offline â†’ Edit data â†’ Online â†’ Updates sync correctly
   - âœ… Offline â†’ Delete data â†’ Online â†’ Deletes sync correctly
   - âœ… Network failure mid-sync â†’ Recovery

3. **Real-Time Sync Journeys** (2 hours)
   - âœ… Device A creates song â†’ Device B sees it (< 1 second)
   - âœ… Device A edits song â†’ Device B sees update
   - âœ… Device A deletes song â†’ Device B removes it
   - âœ… User doesn't see their own changes in toasts
   - âœ… Multiple rapid changes â†’ All sync correctly

4. **Data Consistency Journeys** (2 hours)
   - âœ… Create song offline â†’ Conflict with cloud version â†’ Resolves correctly
   - âœ… Two devices edit same song simultaneously â†’ Last-write-wins works
   - âœ… Version tracking increments correctly
   - âœ… Audit log records all changes

5. **Error Recovery Journeys** (1 hour)
   - âœ… Network error during CREATE â†’ Retries â†’ Succeeds
   - âœ… Network error during UPDATE â†’ Doesn't duplicate
   - âœ… Network error during DELETE â†’ Doesn't lose data
   - âœ… Sync queue failure â†’ Shows error â†’ User can retry

**Deliverable:** 20-30 journey tests covering critical paths

### P1: Edge Cases (Should Have Before MVP)
**Time: 6-8 hours**

1. **Session/Auth Edge Cases** (2 hours)
   - Session expires during song creation
   - Session expires during sync
   - Invalid auth token â†’ Handles gracefully
   - Permission denied â†’ Shows error
   - Rate limiting â†’ Backs off

2. **Network Edge Cases** (2 hours)
   - Intermittent connection (flaky network)
   - Slow 3G connection â†’ Doesn't timeout prematurely
   - WebSocket disconnect â†’ Reconnects automatically
   - API endpoint down â†’ Falls back gracefully
   - CORS errors â†’ Handles gracefully

3. **Data Edge Cases** (2 hours)
   - Invalid date formats â†’ Doesn't crash
   - Missing required fields â†’ Validation catches it
   - Corrupted JSON in audit log â†’ Handles gracefully
   - Large song title (> 500 chars) â†’ Truncates or rejects
   - Special characters in names â†’ Escapes correctly
   - Null values in unexpected places â†’ Handles safely

4. **Concurrency Edge Cases** (2 hours)
   - Multiple tabs open â†’ State syncs between tabs
   - Rapid clicks on save â†’ Doesn't create duplicates
   - Create then immediately delete â†’ Handles correctly
   - Sync queue overflow (100+ pending) â†’ Doesn't crash
   - WebSocket receives changes faster than UI can render â†’ Batches correctly

**Deliverable:** 20-30 edge case tests

### P2: Performance & Stress Tests (Nice to Have)
**Time: 4-6 hours**

1. **Performance Tests** (2 hours)
   - 100 songs â†’ App stays responsive
   - 50 setlists with 20 songs each â†’ No lag
   - Long-running session (8 hours) â†’ No memory leaks
   - 1000 audit log entries â†’ Query performance OK

2. **Stress Tests** (2 hours)
   - Create 100 songs rapidly â†’ All sync
   - 10 devices sync simultaneously â†’ No conflicts
   - Sync 500MB of data â†’ Completes successfully
   - WebSocket receives 100 events/second â†’ Handles load

**Deliverable:** 10-15 performance tests

### P3: Integration Tests (Post-MVP)
**Time: 8-10 hours**

- Component integration tests
- Hook integration tests
- Page-level tests
- Navigation flows

**Deliverable:** 30-40 integration tests

### P4: E2E Tests (Post-MVP)
**Time: 10-12 hours**

- Cypress/Playwright setup
- Visual regression tests
- Full user workflows
- Cross-browser testing

**Deliverable:** 15-20 E2E tests

---

## ğŸ” Test Debt Analysis & Triage

### Delete vs Fix Decision Matrix

| Test Category | Current Status | Recommendation | Reason |
|---------------|----------------|----------------|---------|
| RealtimeManager (8 failing) | Mock setup issues | **FIX** (2-3h) | Core sync functionality, worth fixing |
| Integration tests (6 failing) | Database cleanup issues | **FIX** (1-2h) | Important patterns to validate |
| PracticesPage tests | React setup issues | **DELETE** (10min) | Replace with journey tests |
| useSongs.test.ts | Hook isolation issues | **DELETE** (5min) | Replace with journey tests |
| Old table-based tests | Architecture mismatch | **DELETE** (5min) | Already migrated to audit-first |
| Happy-path unit tests | Pass but low value | **KEEP** | Don't break what works |

### Specific Recommendations

**DELETE THESE (save time, replace with better tests):**
```bash
# Delete outdated/low-value tests
tests/unit/pages/PracticesPage.test.tsx  # Replace with journey tests
tests/unit/hooks/useSongs.test.ts  # Replace with journey tests
# Any other component/hook unit tests that test implementation
```

**FIX THESE (high value, worth the time):**
```bash
# Fix mock setup for core sync tests
tests/unit/services/data/RealtimeManager.test.ts  # 2-3 hours
tests/integration/optimistic-updates.test.ts  # 1 hour
tests/integration/cloud-first-reads.test.ts  # 1 hour
tests/integration/immediate-sync.test.ts  # 1 hour
```

**KEEP THESE (already passing, good coverage):**
```bash
tests/unit/services/data/SyncEngine.test.ts  # 21/21 passing âœ…
tests/unit/services/data/RemoteRepository.test.ts  # 13/13 passing âœ…
tests/unit/services/data/LocalRepository.test.ts  # All passing âœ…
# All other passing sync infrastructure tests
```

---

## ğŸš€ Recommended Implementation Plan

### Phase 1: Quick Wins (2-3 hours) - DO THIS FIRST
**Goal:** Get to 100% on valuable tests, delete low-value tests

1. **Delete low-value tests (30 min)**
   ```bash
   # Remove tests that test implementation details
   rm tests/unit/pages/PracticesPage.test.tsx
   rm tests/unit/hooks/useSongs.test.ts
   # Check for other component/hook unit tests to remove
   ```

2. **Fix high-value failing tests (2-3 hours)**
   - RealtimeManager.test.ts: Fix mock to return unique channel instances
   - Integration tests: Fix database cleanup timing
   - Run full suite: Should be ~100% passing on remaining tests

3. **Document decisions (30 min)**
   - Update test plan with what was deleted and why
   - Document new testing philosophy
   - Create baseline metrics

**Deliverable:** 100% passing test suite (smaller, higher quality)

### Phase 2: Critical Journey Tests (8-10 hours) - BEFORE MVP LAUNCH
**Goal:** Cover real user journeys and edge cases found in production

1. **Setup journey test infrastructure (2 hours)**
   ```typescript
   // tests/journeys/helpers/testSetup.ts
   export async function createTestDevice(name: string) {
     // Setup isolated browser context
     // Initialize database
     // Login user
     return {
       createSong, editSong, deleteSong,
       getSongs, waitForSync,
       goOffline, goOnline,
       simulateSessionTimeout,
       // ... other helpers
     }
   }
   ```

2. **Write critical journey tests (6-8 hours)**
   - Auth journeys (2h)
   - Offline/online journeys (3h)
   - Real-time sync journeys (2h)
   - Error recovery journeys (1h)

3. **Validate tests catch known issues (1 hour)**
   - Session timeout test should fail on current code
   - Fix the bug
   - Test should pass
   - This validates test is useful!

**Deliverable:** 20-30 journey tests covering critical paths

### Phase 3: Edge Case Tests (6-8 hours) - AFTER MVP LAUNCH
**Goal:** Cover edge cases that could break in production

1. **Write edge case tests (6 hours)**
   - Session/auth edge cases (2h)
   - Network edge cases (2h)
   - Data edge cases (2h)
   - Concurrency edge cases (2h)

2. **Fix bugs found by tests (varies)**
   - Run each test
   - If it fails, fix the bug
   - Re-run until passes

**Deliverable:** 20-30 edge case tests

### Phase 4: Performance & Stress Tests (4-6 hours) - POST-MVP
**Goal:** Ensure app can handle real-world load

1. **Write performance tests (2 hours)**
2. **Write stress tests (2 hours)**
3. **Establish performance baselines (1 hour)**

**Deliverable:** 10-15 performance tests with baselines

---

## ğŸ“Š Success Metrics

### Before MVP Launch (Must Have)
- [ ] 100% passing tests on remaining test suite
- [ ] 20-30 journey tests covering critical paths
- [ ] Session timeout edge case has test + fix
- [ ] All user-reported edge cases have tests
- [ ] Developer dashboard shows test results in real-time
- [ ] No test takes > 5 seconds to run
- [ ] Full suite runs in < 2 minutes

### After MVP Launch (Nice to Have)
- [ ] 20-30 edge case tests
- [ ] 10-15 performance tests
- [ ] 90%+ code coverage (but not the goal!)
- [ ] All failing tests from today are either fixed or deleted
- [ ] CI/CD runs tests on every PR
- [ ] Test dashboard integrated into dev tools

---

## ğŸ¯ Key Insights

### What We Learned
1. **High pass rate â‰  Good coverage** - 87% passing but missing critical edge cases
2. **Implementation tests are fragile** - Break on refactors, don't catch real bugs
3. **Journey tests are resilient** - Test what users do, survive refactors
4. **Edge cases found in production** - Session timeout should have had test first
5. **Developer dashboard needed** - Can't validate tests without seeing app state

### What Changed Since Last Plan
1. **Old plan outdated** - Thought we had 8 failing tests, actually have 69
2. **Test philosophy shift** - From "fix all failing tests" to "test the right things"
3. **Delete tests liberally** - Bad tests are worse than no tests
4. **Focus on journeys** - One journey test > 10 unit tests
5. **Edge cases first** - These are what break production

### Recommendations for User
1. **Don't fix all 69 failing tests** - Many are low-value
2. **Delete tests that test implementation** - They're fragile and don't catch bugs
3. **Write journey tests for session timeout** - The edge case you found
4. **Build dev dashboard first** - Need to validate tests against real app state
5. **Get to 100% on valuable tests fast** - Then write new tests from scratch

---

## ğŸ”§ Implementation Tools Needed

### Journey Test Infrastructure
```typescript
// tests/journeys/helpers/testDevice.ts
export class TestDevice {
  name: string
  db: IDBDatabase
  supabase: SupabaseClient
  isOnline: boolean

  async createSong(data: Partial<Song>): Promise<Song>
  async editSong(id: string, data: Partial<Song>): Promise<Song>
  async deleteSong(id: string): Promise<void>
  async getSongs(): Promise<Song[]>

  async goOffline(): Promise<void>
  async goOnline(): Promise<void>
  async waitForSync(timeout?: number): Promise<void>

  async simulateSessionTimeout(): Promise<void>
  async simulateNetworkError(): Promise<void>
  async simulateSlowNetwork(latency: number): Promise<void>
}

// tests/journeys/helpers/testScenario.ts
export class TestScenario {
  devices: Map<string, TestDevice>

  async setupDevices(names: string[]): Promise<void>
  async teardown(): Promise<void>
  async waitForAllDevicesToSync(): Promise<void>
  async verifyDataConsistency(): Promise<boolean>
}
```

### Edge Case Simulation Helpers
```typescript
// tests/helpers/edgeCaseSimulation.ts
export async function simulateSessionTimeout(device: TestDevice)
export async function simulateNetworkFailure(device: TestDevice, duration: number)
export async function simulateSlowNetwork(device: TestDevice, latency: number)
export async function simulateWebSocketDisconnect(device: TestDevice)
export async function simulateRateLimit(device: TestDevice)
export async function simulateInvalidToken(device: TestDevice)
export async function simulateCorruptedData(device: TestDevice, data: any)
```

### Assertion Helpers
```typescript
// tests/helpers/assertions.ts
export async function expectSyncedWithinTimeout(
  device1: TestDevice,
  device2: TestDevice,
  timeout: number = 1000
)

export async function expectDataConsistent(
  devices: TestDevice[]
)

export async function expectNoDataLoss(
  beforeSnapshot: any,
  afterSnapshot: any
)

export async function expectGracefulError(
  errorMessage: RegExp,
  screen: any
)
```

---

## ğŸ“ Next Steps (Immediate Actions)

### For You (User) to Decide:

**Question 1: Test Philosophy**
Do you agree with the "delete low-value tests, write journey tests from scratch" approach?
- **Option A:** Yes, delete and start fresh (RECOMMENDED)
- **Option B:** No, fix all 69 failing tests first

**Question 2: Priority Order**
What order should we tackle this?
- **Option A:** Quick wins â†’ Journey tests â†’ Dashboard (RECOMMENDED)
- **Option B:** Dashboard â†’ Journey tests â†’ Quick wins
- **Option C:** Journey tests â†’ Dashboard â†’ Quick wins

**Question 3: Session Timeout Test**
Should we write a test for the session timeout edge case you found?
- **Option A:** Yes, write test first then fix bug (TDD) (RECOMMENDED)
- **Option B:** Fix bug first, then write test
- **Option C:** Just fix bug, skip test

### Recommended Immediate Actions (Next 2-3 hours):

1. **Delete low-value tests (30 min)**
   - Remove component/hook unit tests that test implementation
   - Remove any deprecated tests from old architecture
   - Get quick win on pass rate

2. **Fix high-value failing tests (2-3 hours)**
   - RealtimeManager: Fix mock setup
   - Integration tests: Fix database cleanup
   - Run full suite to validate 100% passing

3. **Create journey test for session timeout (30 min)**
   - Write the test first (it should fail)
   - This validates the edge case you found
   - Proves our new testing approach catches real bugs

4. **Update test plan (30 min)**
   - Document what was deleted and why
   - Update roadmap with new strategy
   - Create completion report

**Total Time to 100% + Session Timeout Test:** 3-4 hours

---

## ğŸ“š Related Documents

### Read These First
- `.claude/specifications/unified-database-schema.md` - Schema reference
- `.claude/artifacts/2025-11-02T05:04_mvp-readiness-assessment.md` - MVP state

### Update These After
- `.claude/instructions/04-remaining-test-fixes-plan.md` - Mark as REPLACED
- `.claude/artifacts/2025-10-29T16:15_unified-implementation-roadmap.md` - Update Phase 4b
- Create new: `.claude/artifacts/2025-11-03T15:20_test-implementation-progress.md`

### Reference Later
- `.claude/artifacts/2025-11-02T05:15_test-update-progress-report.md` - Previous test work
- `.claude/artifacts/2025-11-02T03:02_testing-and-quality-improvement-plan.md` - Old strategy

---

## ğŸ¯ Final Recommendation

**STOP fixing implementation tests. START writing journey tests.**

**Why:**
1. âœ… Session timeout bug you found â†’ Should have had journey test
2. âœ… 69 failing tests â†’ Many are low-value implementation tests
3. âœ… 555 passing tests â†’ Give false sense of security, didn't catch real bug
4. âœ… Journey tests catch real bugs â†’ More valuable than 100% pass rate
5. âœ… MVP launch this week â†’ Need tests that matter, not tests that pass

**Timeline to MVP:**
- **Phase 1 (Quick Wins):** 2-3 hours â†’ 100% passing on valuable tests
- **Phase 2 (Journey Tests):** 8-10 hours â†’ Critical path coverage
- **Developer Dashboard:** 6-8 hours â†’ Validation tool
- **Total:** 16-21 hours to MVP confidence

**You were absolutely right to focus on edge cases and user journeys. This new strategy does exactly that!**

---

**Status:** Ready for Implementation â­
**Created:** 2025-11-03T15:20
**Next Update:** After Phase 1 completion (quick wins)
**Time to 100% Valuable Tests:** 2-3 hours
**Time to MVP Confidence:** 16-21 hours
